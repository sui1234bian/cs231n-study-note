{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDJwQPZcupab"
   },
   "source": [
    "# EECS 498-007/598-005 Assignment 5-1: Image captioning with RNNs and LSTMs\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KMxqLt1h2kx"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hello WORLD, #sui1234bian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Euek3FWn6bhA",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image Captioning with RNNs\n",
    "\n",
    "In this exercise, you will implement vanilla recurrent neural networks (RNNs), [long-short term memory networks (LSTMs)](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory), and [attention-based LSTMs](https://arxiv.org/abs/1409.0473) to train a model that can generate natural language captions for images.\n",
    "\n",
    "Models in this exercise are highly similar to very early works in neural-network based image captioning. If you are interested to learn more, check out these two papers:\n",
    "\n",
    "1. [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)\n",
    "2. [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzqbYcKdz6ew"
   },
   "source": [
    "## Setup Code\n",
    "\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You\"ll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "28O_qwFfdQpr"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3H9pcnudWlg"
   },
   "source": [
    "### Google Colab Setup\n",
    "\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yv8Z8EiudX25"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sek0GtVOdlKT"
   },
   "source": [
    " Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"eecs598\", \"a5_helper.py\", \"rnn_lstm_captioning.ipynb\",  \"rnn_lstm_captioning.py\", \"Transformers.py\", \"Transformers.ipynb\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A9t0-bGZdnr8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# # TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# # Example: If you create a WI2022 folder and put all the files under A5 folder, then \"WI2022/A5\"\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "# GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "# print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "\n",
    "# # Add to sys so we can import .py files.\n",
    "# sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5LWJPBtdrpZ"
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from rnn_lstm_captioning.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `rnn_lstm_captioning.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFZKi0podzhO"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# from rnn_lstm_captioning import hello_rnn_lstm_captioning\n",
    "\n",
    "\n",
    "# os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "# time.tzset()\n",
    "# hello_rnn_lstm_captioning()\n",
    "\n",
    "# rnn_lstm_path = os.path.join(GOOGLE_DRIVE_PATH, \"rnn_lstm_captioning.py\")\n",
    "# rnn_lstm_edit_time = time.ctime(os.path.getmtime(rnn_lstm_path))\n",
    "# print(\"rnn_lstm_captioning.py last edited on %s\" % rnn_lstm_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8fTwRpXfwyM"
   },
   "source": [
    "### Load Packages\n",
    "\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q53DlMXboP-T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "C:\\Users\\24813\\AppData\\Local\\Temp\\ipykernel_41952\\942963187.py:14: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn\")  # Prettier plots\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from eecs598.grad import compute_numeric_gradient, rel_error\n",
    "from eecs598.utils import attention_visualizer, reset_seed\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn\")  # Prettier plots\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"font.size\"] = 24\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvUDZWGU3VLV"
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RrAX9FOLpr9k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Define some common variables for dtypes/devices.\n",
    "# These can be keyword arguments while defining new tensors.\n",
    "to_float = {\"dtype\": torch.float32, \"device\": DEVICE}\n",
    "to_double = {\"dtype\": torch.float64, \"device\": DEVICE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCPZwvOd6bhF",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# COCO Captions\n",
    "\n",
    "For this exercise we will use the 2014 release of the [COCO Captions dataset](http://cocodataset.org/) which has become the standard testbed for image captioning. The dataset consists of 80,000 training images and 40,000 validation images, each annotated with 5 captions written by workers on Amazon Mechanical Turk.\n",
    "\n",
    "We have preprocessed the data for you already and saved them into a serialized data file. It contains 10,000 image-caption pairs for training and 500 for testing. The images have been downsampled to 112x112 for computation efficiency and captions are tokenized and numericalized, clamped to 15 words. You can download the file named `coco.pt` (378MB) with the link below and run some useful stats.\n",
    "\n",
    "You will later use RegNet-X 400MF model to extract features for the images. A few notes on the caption preprocessing:\n",
    "\n",
    "Dealing with strings is inefficient, so we will work with an encoded version of the captions. Each word is assigned an integer ID, allowing us to represent a caption by a sequence of integers. The mapping between integer IDs and words is saved in an entry named `vocab` (both `idx_to_token` and `token_to_idx`), and we use the function `decode_captions` from `a5_helper.py` to convert tensors of integer IDs back into strings.\n",
    "\n",
    "There are a couple special tokens that we add to the vocabulary. We prepend a special `<START>` token and append an `<END>` token to the beginning and end of each caption respectively. Rare words are replaced with a special `<UNK>` token (for \"unknown\"). In addition, since we want to train with minibatches containing captions of different lengths, we pad short captions with a special `<NULL>` token after the `<END>` token and don't compute loss or gradient for `<NULL>` tokens. Since they are a bit of a pain, we have taken care of all implementation details around special tokens for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set a few constants related to data loading.\n",
    "IMAGE_SHAPE = (112, 112)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "# Batch size used for full training runs:\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch size used for overfitting sanity checks:\n",
    "OVR_BATCH_SIZE = BATCH_SIZE // 8\n",
    "\n",
    "# Batch size used for visualization:\n",
    "VIS_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IMok4gFXqjre"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data exists!\n",
      "train_images <class 'torch.Tensor'> torch.Size([10000, 3, 112, 112]) torch.uint8\n",
      "train_captions <class 'torch.Tensor'> torch.Size([10000, 17]) torch.int64\n",
      "val_images <class 'torch.Tensor'> torch.Size([500, 3, 112, 112]) torch.uint8\n",
      "val_captions <class 'torch.Tensor'> torch.Size([500, 17]) torch.int64\n",
      "vocab <class 'dict'> dict_keys(['idx_to_token', 'token_to_idx'])\n",
      "\n",
      "Train images shape:  torch.Size([10000, 3, 112, 112])\n",
      "Train caption tokens shape:  torch.Size([10000, 17])\n",
      "Validation images shape:  torch.Size([500, 3, 112, 112])\n",
      "Validation caption tokens shape:  torch.Size([500, 17])\n",
      "total number of caption tokens:  864\n",
      "mappings (list) from index to caption token:  ['<NULL>', '<START>', '<END>', '<UNK>', 'a', 'on', 'of', 'the', 'in', 'with', 'and', 'is', 'man', 'to', 'sitting', 'two', 'an', 'standing', 'people', 'are', 'at', 'next', 'white', 'woman', 'table', 'that', 'street', 'holding', 'some', 'person', 'down', 'large', 'top', 'group', 'tennis', 'field', 'it', 'plate', 'up', 'small', 'riding', 'room', 'front', 'near', 'dog', 'red', 'his', 'by', 'black', 'train', 'baseball', 'young', 'cat', 'water', 'walking', 'playing', 'sign', 'snow', 'while', 'pizza', 'has', 'bathroom', 'kitchen', 'there', 'bus', 'grass', 'food', 'blue', 'green', 'other', 'beach', 'couple', 'ball', 'building', 'bed', 'three', 'parked', 'men', 'for', 'flying', 'side', 'looking', 'wooden', 'toilet', 'game', 'road', 'boy', 'girl', 'player', 'laying', 'skateboard', 'city', 'sits', 'over', 'wearing', 'her', 'eating', 'frisbee', 'several', 'out', 'bear', 'through', 'sink', 'horse', 'outside', 'picture', 'giraffe', 'from', 'phone', 'around', 'wall', 'bench', 'air', 'each', 'brown', 'board', 'clock', 'yellow', 'window', 'laptop', 'one', 'its', 'car', 'area', 'under', 'stop', 'park', 'living', 'covered', 'cake', 'behind', 'court', 'their', 'open', 'kite', 'into', 'elephant', 'truck', 'umbrella', 'tree', 'this', 'airplane', 'very', 'sheep', 'surfboard', 'many', 'trees', 'close', 'filled', 'little', 'old', 'computer', 'skis', 'motorcycle', 'big', 'desk', 'together', 'bowl', 'light', 'sky', 'as', 'bunch', 'background', 'wave', 'chair', 'traffic', 'teddy', 'fire', 'counter', 'ocean', 'sandwich', 'plane', 'cell', 'inside', 'glass', 'giraffes', 'sidewalk', 'stands', 'child', 'boat', 'back', 'women', 'orange', 'cars', 'photo', 'bat', 'horses', 'skiing', 'couch', 'baby', 'zebras', 'fence', 'bird', 'sit', 'racket', 'hydrant', 'view', 'bananas', 'grassy', 'elephants', 'stand', 'shirt', 'middle', 'vegetables', 'hill', 'four', 'flowers', 'tie', 'tall', 'hand', 'vase', 'off', 'grazing', 'driving', 'different', 'zebra', 'bike', 'being', 'ground', 'mirror', 'full', 'hanging', 'another', 'tracks', 'slope', 'dirt', 'along', 'ready', 'mountain', 'lot', 'wine', 'station', 'talking', 'cows', 'taking', 'skate', 'stuffed', 'during', 'day', 'floor', 'swinging', 'signs', 'pink', 'herd', 'airport', 'ski', 'head', 'guy', 'glasses', 'display', 'cutting', 'above', 'image', 'fruit', 'refrigerator', 'holds', 'going', 'empty', 'cow', 'broccoli', 'wii', 'pair', 'long', 'colorful', 'beside', 'track', 'surf', 'stove', 'pole', 'parking', 'crowd', 'against', 'tower', 'luggage', 'dogs', 'snowy', 'runway', 'lots', 'hat', 'umbrellas', 'smiling', 'scissors', 'kites', 'getting', 'buildings', 'walk', 'using', 'chairs', 'animals', 'skier', 'racquet', 'posing', 'passenger', 'paper', 'corner', 'banana', 'across', 'topped', 'them', 'running', 'piece', 'night', 'lights', 'jumping', 'hot', 'hit', 'video', 'tv', 'looks', 'carrying', 'suit', 'remote', 'oven', 'home', 'doing', 'box', 'body', 'birds', 'batter', 'television', 'plates', 'house', 'children', 'camera', 'busy', 'boats', 'various', 'soccer', 'motorcycles', 'jet', 'cheese', 'bears', 'shower', 'metal', 'male', 'double', 'bedroom', 'wood', 'trick', 'skiers', 'sand', 'rides', 'traveling', 'dark', 'be', 'snowboard', 'microwave', 'lady', 'keyboard', 'items', 'he', 'drinking', 'door', 'way', 'tray', 'river', 'restaurant', 'players', 'meat', 'like', 'set', 'line', 'kids', 'cup', 'all', 'about', 'watching', 'bridge', 'brick', 'book', 'toy', 'skateboarder', 'photograph', 'made', 'kid', 'coffee', 'bread', 'boys', 'surfer', 'shown', 'row', 'ramp', 'face', 'donuts', 'cut', 'cross', 'something', 'preparing', 'market', 'lake', 'half', 'dressed', 'decker', 'who', 'tub', 'surrounded', 'suitcase', 'slice', 'oranges', 'lying', 'lush', 'knife', 'him', 'gray', 'furniture', 'forest', 'enclosure', 'bicycle', 'bath', 'scene', 'purple', 'play', 'number', 'hands', 'bottle', 'beautiful', 'swing', 'screen', 'pulling', 'past', 'leaning', 'jacket', 'female', 'country', 'carrots', 'cabinets', 'animal', 'walks', 'waiting', 'shelf', 'pan', 'older', 'making', 'look', 'leaves', 'bag', 'zoo', 'someone', 'snowboarder', 'mouth', 'meal', 'grey', 'dress', 'controller', 'base', 'intersection', 'fries', 'few', 'displayed', 'clean', 'throwing', 'sun', 'store', 'stone', 'seen', 'police', 'onto', 'decorated', 'cute', 'chocolate', 'buses', 'between', 'variety', 'silver', 'showing', 'salad', 'rocks', 'lined', 'high', 'girls', 'fruits', 'engine', 'birthday', 'attached', 'adult', 'waves', 'trying', 'slices', 'sleeping', 'skateboarding', 'pitch', 'pile', 'pictures', 'painted', 'multiple', 'lit', 'hair', 'catch', 'types', 'surfing', 'stopped', 'setting', 'placed', 'outdoor', 'mounted', 'motor', 'helmet', 'gear', 'windows', 'underneath', 'tables', 'sofa', 'seat', 'resting', 'public', 'pizzas', 'moving', 'mouse', 'lap', 'hotel', 'hitting', 'flower', 'edge', 'cats', 'brushing', 'been', 'apple', 'working', 'uniform', 'shot', 'rock', 'ride', 'performing', 'passing', 'nearby', 'mountains', 'graffiti', 'floating', 'flies', 'five', 'drink', 'coming', 'colored', 'clocks', 'case', 'carriage', 'branch', 'books', 'bathtub', 'apples', 'tarmac', 'rain', 'others', 'office', 'just', 'have', 'fly', 'eaten', 'eat', 'doughnut', 'dish', 'cloudy', 'clear', 'bright', 'watches', 'walls', 'vases', 'trunk', 'tiled', 'teeth', 'she', 'school', 'sandy', 'plant', 'perched', 'pasture', 'or', 'modern', 'military', 'donut', 'dirty', 'dinner', 'desert', 'catcher', 'bikes', 'yard', 'wet', 'watch', 'tricks', 'toppings', 'toothbrush', 'surface', 'statue', 'shaped', 'scooter', 'sandwiches', 'rail', 'plastic', 'pieces', 'path', 'neck', 'mother', 'match', 'machine', 'legs', 'having', 'gathered', 'fork', 'crowded', 'crossing', 'containing', 'church', 'cart', 'candles', 'bushes', 'boxes', 'blender', 'beer', 'alone', 'vehicles', 'vehicle', 'towards', 'sunglasses', 'stairs', 'shop', 'rice', 'reading', 'plays', 'pitcher', 'not', 'nintendo', 'foods', 'fireplace', 'fenced', 'enjoying', 'end', 'dry', 'dining', 'curb', 'computers', 'christmas', 'center', 'cellphone', 'cattle', 'catching', 'brush', 'boards', 'boarding', 'blanket', 'bicycles', 'basket', 'bags', 'wild', 'trail', 'town', 'style', 'stacked', 'smiles', 'sliced', 'served', 'sauce', 'pulled', 'professional', 'pretty', 'pots', 'platform', 'place', 'phones', 'meter', 'hillside', 'french', 'feeding', 'distance', 'control', 'chicken', 'cement', 'can', 'bowls', 'bottles', 'boarder', 'benches', 'assortment', 'towel', 'toward', 'toddler', 'tile', 'taken', 'sunny', 'striped', 'sticking', 'sinks', 'single', 'shoes', 'rests', 'railroad', 'putting', 'potatoes', 'plants', 'planes', 'pillows', 'pen', 'no', 'landing', 'jumps', 'jump', 'huge', 'hotdog', 'fridge', 'fish', 'event', 'dock', 'cream', 'collection', 'clothes', 'business', 'atop', 'antique', 'wooded', 'wire', 'transit', 'things', 'steam', 'skateboards', 'serve', 'reaching', 'pool', 'pie', 'persons', 'overhead', 'monitor', 'mid', 'guys', 'graze', 'flag', 'equipment', 'doughnuts', 'cooking', 'cooked', 'container', 'cluttered', 'away', 'arm', 'appliances', 'vest', 'type', 'trains', 'toilets', 'steps', 'square', 'spoon', 'shore', 'shade', 'sale', 'run', 'right', 'rack', 'prepares', 'polar', 'picnic', 'party', 'outdoors', 'officer', 'mound', 'ledge', 'kind', 'hay', 'gold', 'giving', 'feet', 'fancy', 'couches', 'concrete', 'closeup', 'broken', 'asian', 'among', 'adults', 'action', 'woods', 'where', 'vintage', 'trucks', 'time', 'throw', 'tan', 'takes', 'take', 'surfers', 'surfboards', 'snowboarding', 'smoke', 'serving', 'rug', 'roof', 'pushing', 'pot', 'pond', 'poles', 'pastries', 'passengers', 'pants', 'overlooking', 'nice', 'mans', 'makes', 'low', 'laptops', 'guitar', 'growing', 'go', 'glove', 'fresh', 'flat', 'family', 'doors', 'dessert', 'colors', 'bush', 'bunches', 'both', 'blurry', 'bite', 'below', 'beds', 'bar', 'after', 'work', 'winter', 'travelling', 'tow', 'throws', 'swimming', 'supplies', 'suitcases', 'subway', 'still', 'space', 'soup', 'someones', 'skies', 'show', 'sheet', 'round', 'railing', 'pulls', 'pose', 'petting', 'pepperoni', 'pedestrians', 'narrow', 'lamp', 'ketchup', 'juice', 'island', 'ice', 'himself', 'hard', 'giant', 'get', 'games', 'eyes', 'drinks', 'displaying', 'decorative', 'curtain', 'coat', 'close-up', 'chips', 'chasing', 'carrot', 'cabinet', 'backpack', 'airplanes', 'airliner']\n",
      "mappings (dict) from caption token to index:  {'<NULL>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3, 'a': 4, 'on': 5, 'of': 6, 'the': 7, 'in': 8, 'with': 9, 'and': 10, 'is': 11, 'man': 12, 'to': 13, 'sitting': 14, 'two': 15, 'an': 16, 'standing': 17, 'people': 18, 'are': 19, 'at': 20, 'next': 21, 'white': 22, 'woman': 23, 'table': 24, 'that': 25, 'street': 26, 'holding': 27, 'some': 28, 'person': 29, 'down': 30, 'large': 31, 'top': 32, 'group': 33, 'tennis': 34, 'field': 35, 'it': 36, 'plate': 37, 'up': 38, 'small': 39, 'riding': 40, 'room': 41, 'front': 42, 'near': 43, 'dog': 44, 'red': 45, 'his': 46, 'by': 47, 'black': 48, 'train': 49, 'baseball': 50, 'young': 51, 'cat': 52, 'water': 53, 'walking': 54, 'playing': 55, 'sign': 56, 'snow': 57, 'while': 58, 'pizza': 59, 'has': 60, 'bathroom': 61, 'kitchen': 62, 'there': 63, 'bus': 64, 'grass': 65, 'food': 66, 'blue': 67, 'green': 68, 'other': 69, 'beach': 70, 'couple': 71, 'ball': 72, 'building': 73, 'bed': 74, 'three': 75, 'parked': 76, 'men': 77, 'for': 78, 'flying': 79, 'side': 80, 'looking': 81, 'wooden': 82, 'toilet': 83, 'game': 84, 'road': 85, 'boy': 86, 'girl': 87, 'player': 88, 'laying': 89, 'skateboard': 90, 'city': 91, 'sits': 92, 'over': 93, 'wearing': 94, 'her': 95, 'eating': 96, 'frisbee': 97, 'several': 98, 'out': 99, 'bear': 100, 'through': 101, 'sink': 102, 'horse': 103, 'outside': 104, 'picture': 105, 'giraffe': 106, 'from': 107, 'phone': 108, 'around': 109, 'wall': 110, 'bench': 111, 'air': 112, 'each': 113, 'brown': 114, 'board': 115, 'clock': 116, 'yellow': 117, 'window': 118, 'laptop': 119, 'one': 120, 'its': 121, 'car': 122, 'area': 123, 'under': 124, 'stop': 125, 'park': 126, 'living': 127, 'covered': 128, 'cake': 129, 'behind': 130, 'court': 131, 'their': 132, 'open': 133, 'kite': 134, 'into': 135, 'elephant': 136, 'truck': 137, 'umbrella': 138, 'tree': 139, 'this': 140, 'airplane': 141, 'very': 142, 'sheep': 143, 'surfboard': 144, 'many': 145, 'trees': 146, 'close': 147, 'filled': 148, 'little': 149, 'old': 150, 'computer': 151, 'skis': 152, 'motorcycle': 153, 'big': 154, 'desk': 155, 'together': 156, 'bowl': 157, 'light': 158, 'sky': 159, 'as': 160, 'bunch': 161, 'background': 162, 'wave': 163, 'chair': 164, 'traffic': 165, 'teddy': 166, 'fire': 167, 'counter': 168, 'ocean': 169, 'sandwich': 170, 'plane': 171, 'cell': 172, 'inside': 173, 'glass': 174, 'giraffes': 175, 'sidewalk': 176, 'stands': 177, 'child': 178, 'boat': 179, 'back': 180, 'women': 181, 'orange': 182, 'cars': 183, 'photo': 184, 'bat': 185, 'horses': 186, 'skiing': 187, 'couch': 188, 'baby': 189, 'zebras': 190, 'fence': 191, 'bird': 192, 'sit': 193, 'racket': 194, 'hydrant': 195, 'view': 196, 'bananas': 197, 'grassy': 198, 'elephants': 199, 'stand': 200, 'shirt': 201, 'middle': 202, 'vegetables': 203, 'hill': 204, 'four': 205, 'flowers': 206, 'tie': 207, 'tall': 208, 'hand': 209, 'vase': 210, 'off': 211, 'grazing': 212, 'driving': 213, 'different': 214, 'zebra': 215, 'bike': 216, 'being': 217, 'ground': 218, 'mirror': 219, 'full': 220, 'hanging': 221, 'another': 222, 'tracks': 223, 'slope': 224, 'dirt': 225, 'along': 226, 'ready': 227, 'mountain': 228, 'lot': 229, 'wine': 230, 'station': 231, 'talking': 232, 'cows': 233, 'taking': 234, 'skate': 235, 'stuffed': 236, 'during': 237, 'day': 238, 'floor': 239, 'swinging': 240, 'signs': 241, 'pink': 242, 'herd': 243, 'airport': 244, 'ski': 245, 'head': 246, 'guy': 247, 'glasses': 248, 'display': 249, 'cutting': 250, 'above': 251, 'image': 252, 'fruit': 253, 'refrigerator': 254, 'holds': 255, 'going': 256, 'empty': 257, 'cow': 258, 'broccoli': 259, 'wii': 260, 'pair': 261, 'long': 262, 'colorful': 263, 'beside': 264, 'track': 265, 'surf': 266, 'stove': 267, 'pole': 268, 'parking': 269, 'crowd': 270, 'against': 271, 'tower': 272, 'luggage': 273, 'dogs': 274, 'snowy': 275, 'runway': 276, 'lots': 277, 'hat': 278, 'umbrellas': 279, 'smiling': 280, 'scissors': 281, 'kites': 282, 'getting': 283, 'buildings': 284, 'walk': 285, 'using': 286, 'chairs': 287, 'animals': 288, 'skier': 289, 'racquet': 290, 'posing': 291, 'passenger': 292, 'paper': 293, 'corner': 294, 'banana': 295, 'across': 296, 'topped': 297, 'them': 298, 'running': 299, 'piece': 300, 'night': 301, 'lights': 302, 'jumping': 303, 'hot': 304, 'hit': 305, 'video': 306, 'tv': 307, 'looks': 308, 'carrying': 309, 'suit': 310, 'remote': 311, 'oven': 312, 'home': 313, 'doing': 314, 'box': 315, 'body': 316, 'birds': 317, 'batter': 318, 'television': 319, 'plates': 320, 'house': 321, 'children': 322, 'camera': 323, 'busy': 324, 'boats': 325, 'various': 326, 'soccer': 327, 'motorcycles': 328, 'jet': 329, 'cheese': 330, 'bears': 331, 'shower': 332, 'metal': 333, 'male': 334, 'double': 335, 'bedroom': 336, 'wood': 337, 'trick': 338, 'skiers': 339, 'sand': 340, 'rides': 341, 'traveling': 342, 'dark': 343, 'be': 344, 'snowboard': 345, 'microwave': 346, 'lady': 347, 'keyboard': 348, 'items': 349, 'he': 350, 'drinking': 351, 'door': 352, 'way': 353, 'tray': 354, 'river': 355, 'restaurant': 356, 'players': 357, 'meat': 358, 'like': 359, 'set': 360, 'line': 361, 'kids': 362, 'cup': 363, 'all': 364, 'about': 365, 'watching': 366, 'bridge': 367, 'brick': 368, 'book': 369, 'toy': 370, 'skateboarder': 371, 'photograph': 372, 'made': 373, 'kid': 374, 'coffee': 375, 'bread': 376, 'boys': 377, 'surfer': 378, 'shown': 379, 'row': 380, 'ramp': 381, 'face': 382, 'donuts': 383, 'cut': 384, 'cross': 385, 'something': 386, 'preparing': 387, 'market': 388, 'lake': 389, 'half': 390, 'dressed': 391, 'decker': 392, 'who': 393, 'tub': 394, 'surrounded': 395, 'suitcase': 396, 'slice': 397, 'oranges': 398, 'lying': 399, 'lush': 400, 'knife': 401, 'him': 402, 'gray': 403, 'furniture': 404, 'forest': 405, 'enclosure': 406, 'bicycle': 407, 'bath': 408, 'scene': 409, 'purple': 410, 'play': 411, 'number': 412, 'hands': 413, 'bottle': 414, 'beautiful': 415, 'swing': 416, 'screen': 417, 'pulling': 418, 'past': 419, 'leaning': 420, 'jacket': 421, 'female': 422, 'country': 423, 'carrots': 424, 'cabinets': 425, 'animal': 426, 'walks': 427, 'waiting': 428, 'shelf': 429, 'pan': 430, 'older': 431, 'making': 432, 'look': 433, 'leaves': 434, 'bag': 435, 'zoo': 436, 'someone': 437, 'snowboarder': 438, 'mouth': 439, 'meal': 440, 'grey': 441, 'dress': 442, 'controller': 443, 'base': 444, 'intersection': 445, 'fries': 446, 'few': 447, 'displayed': 448, 'clean': 449, 'throwing': 450, 'sun': 451, 'store': 452, 'stone': 453, 'seen': 454, 'police': 455, 'onto': 456, 'decorated': 457, 'cute': 458, 'chocolate': 459, 'buses': 460, 'between': 461, 'variety': 462, 'silver': 463, 'showing': 464, 'salad': 465, 'rocks': 466, 'lined': 467, 'high': 468, 'girls': 469, 'fruits': 470, 'engine': 471, 'birthday': 472, 'attached': 473, 'adult': 474, 'waves': 475, 'trying': 476, 'slices': 477, 'sleeping': 478, 'skateboarding': 479, 'pitch': 480, 'pile': 481, 'pictures': 482, 'painted': 483, 'multiple': 484, 'lit': 485, 'hair': 486, 'catch': 487, 'types': 488, 'surfing': 489, 'stopped': 490, 'setting': 491, 'placed': 492, 'outdoor': 493, 'mounted': 494, 'motor': 495, 'helmet': 496, 'gear': 497, 'windows': 498, 'underneath': 499, 'tables': 500, 'sofa': 501, 'seat': 502, 'resting': 503, 'public': 504, 'pizzas': 505, 'moving': 506, 'mouse': 507, 'lap': 508, 'hotel': 509, 'hitting': 510, 'flower': 511, 'edge': 512, 'cats': 513, 'brushing': 514, 'been': 515, 'apple': 516, 'working': 517, 'uniform': 518, 'shot': 519, 'rock': 520, 'ride': 521, 'performing': 522, 'passing': 523, 'nearby': 524, 'mountains': 525, 'graffiti': 526, 'floating': 527, 'flies': 528, 'five': 529, 'drink': 530, 'coming': 531, 'colored': 532, 'clocks': 533, 'case': 534, 'carriage': 535, 'branch': 536, 'books': 537, 'bathtub': 538, 'apples': 539, 'tarmac': 540, 'rain': 541, 'others': 542, 'office': 543, 'just': 544, 'have': 545, 'fly': 546, 'eaten': 547, 'eat': 548, 'doughnut': 549, 'dish': 550, 'cloudy': 551, 'clear': 552, 'bright': 553, 'watches': 554, 'walls': 555, 'vases': 556, 'trunk': 557, 'tiled': 558, 'teeth': 559, 'she': 560, 'school': 561, 'sandy': 562, 'plant': 563, 'perched': 564, 'pasture': 565, 'or': 566, 'modern': 567, 'military': 568, 'donut': 569, 'dirty': 570, 'dinner': 571, 'desert': 572, 'catcher': 573, 'bikes': 574, 'yard': 575, 'wet': 576, 'watch': 577, 'tricks': 578, 'toppings': 579, 'toothbrush': 580, 'surface': 581, 'statue': 582, 'shaped': 583, 'scooter': 584, 'sandwiches': 585, 'rail': 586, 'plastic': 587, 'pieces': 588, 'path': 589, 'neck': 590, 'mother': 591, 'match': 592, 'machine': 593, 'legs': 594, 'having': 595, 'gathered': 596, 'fork': 597, 'crowded': 598, 'crossing': 599, 'containing': 600, 'church': 601, 'cart': 602, 'candles': 603, 'bushes': 604, 'boxes': 605, 'blender': 606, 'beer': 607, 'alone': 608, 'vehicles': 609, 'vehicle': 610, 'towards': 611, 'sunglasses': 612, 'stairs': 613, 'shop': 614, 'rice': 615, 'reading': 616, 'plays': 617, 'pitcher': 618, 'not': 619, 'nintendo': 620, 'foods': 621, 'fireplace': 622, 'fenced': 623, 'enjoying': 624, 'end': 625, 'dry': 626, 'dining': 627, 'curb': 628, 'computers': 629, 'christmas': 630, 'center': 631, 'cellphone': 632, 'cattle': 633, 'catching': 634, 'brush': 635, 'boards': 636, 'boarding': 637, 'blanket': 638, 'bicycles': 639, 'basket': 640, 'bags': 641, 'wild': 642, 'trail': 643, 'town': 644, 'style': 645, 'stacked': 646, 'smiles': 647, 'sliced': 648, 'served': 649, 'sauce': 650, 'pulled': 651, 'professional': 652, 'pretty': 653, 'pots': 654, 'platform': 655, 'place': 656, 'phones': 657, 'meter': 658, 'hillside': 659, 'french': 660, 'feeding': 661, 'distance': 662, 'control': 663, 'chicken': 664, 'cement': 665, 'can': 666, 'bowls': 667, 'bottles': 668, 'boarder': 669, 'benches': 670, 'assortment': 671, 'towel': 672, 'toward': 673, 'toddler': 674, 'tile': 675, 'taken': 676, 'sunny': 677, 'striped': 678, 'sticking': 679, 'sinks': 680, 'single': 681, 'shoes': 682, 'rests': 683, 'railroad': 684, 'putting': 685, 'potatoes': 686, 'plants': 687, 'planes': 688, 'pillows': 689, 'pen': 690, 'no': 691, 'landing': 692, 'jumps': 693, 'jump': 694, 'huge': 695, 'hotdog': 696, 'fridge': 697, 'fish': 698, 'event': 699, 'dock': 700, 'cream': 701, 'collection': 702, 'clothes': 703, 'business': 704, 'atop': 705, 'antique': 706, 'wooded': 707, 'wire': 708, 'transit': 709, 'things': 710, 'steam': 711, 'skateboards': 712, 'serve': 713, 'reaching': 714, 'pool': 715, 'pie': 716, 'persons': 717, 'overhead': 718, 'monitor': 719, 'mid': 720, 'guys': 721, 'graze': 722, 'flag': 723, 'equipment': 724, 'doughnuts': 725, 'cooking': 726, 'cooked': 727, 'container': 728, 'cluttered': 729, 'away': 730, 'arm': 731, 'appliances': 732, 'vest': 733, 'type': 734, 'trains': 735, 'toilets': 736, 'steps': 737, 'square': 738, 'spoon': 739, 'shore': 740, 'shade': 741, 'sale': 742, 'run': 743, 'right': 744, 'rack': 745, 'prepares': 746, 'polar': 747, 'picnic': 748, 'party': 749, 'outdoors': 750, 'officer': 751, 'mound': 752, 'ledge': 753, 'kind': 754, 'hay': 755, 'gold': 756, 'giving': 757, 'feet': 758, 'fancy': 759, 'couches': 760, 'concrete': 761, 'closeup': 762, 'broken': 763, 'asian': 764, 'among': 765, 'adults': 766, 'action': 767, 'woods': 768, 'where': 769, 'vintage': 770, 'trucks': 771, 'time': 772, 'throw': 773, 'tan': 774, 'takes': 775, 'take': 776, 'surfers': 777, 'surfboards': 778, 'snowboarding': 779, 'smoke': 780, 'serving': 781, 'rug': 782, 'roof': 783, 'pushing': 784, 'pot': 785, 'pond': 786, 'poles': 787, 'pastries': 788, 'passengers': 789, 'pants': 790, 'overlooking': 791, 'nice': 792, 'mans': 793, 'makes': 794, 'low': 795, 'laptops': 796, 'guitar': 797, 'growing': 798, 'go': 799, 'glove': 800, 'fresh': 801, 'flat': 802, 'family': 803, 'doors': 804, 'dessert': 805, 'colors': 806, 'bush': 807, 'bunches': 808, 'both': 809, 'blurry': 810, 'bite': 811, 'below': 812, 'beds': 813, 'bar': 814, 'after': 815, 'work': 816, 'winter': 817, 'travelling': 818, 'tow': 819, 'throws': 820, 'swimming': 821, 'supplies': 822, 'suitcases': 823, 'subway': 824, 'still': 825, 'space': 826, 'soup': 827, 'someones': 828, 'skies': 829, 'show': 830, 'sheet': 831, 'round': 832, 'railing': 833, 'pulls': 834, 'pose': 835, 'petting': 836, 'pepperoni': 837, 'pedestrians': 838, 'narrow': 839, 'lamp': 840, 'ketchup': 841, 'juice': 842, 'island': 843, 'ice': 844, 'himself': 845, 'hard': 846, 'giant': 847, 'get': 848, 'games': 849, 'eyes': 850, 'drinks': 851, 'displaying': 852, 'decorative': 853, 'curtain': 854, 'coat': 855, 'close-up': 856, 'chips': 857, 'chasing': 858, 'carrot': 859, 'cabinet': 860, 'backpack': 861, 'airplanes': 862, 'airliner': 863}\n"
     ]
    }
   ],
   "source": [
    "from a5_helper import load_coco_captions\n",
    "\n",
    "# Download and load serialized COCO data from coco.pt\n",
    "# It contains a dictionary of\n",
    "# \"train_images\" - resized training images (IMAGE_SHAPE)\n",
    "# \"val_images\" - resized validation images (IMAGE_SHAPE)\n",
    "# \"train_captions\" - tokenized and numericalized training captions\n",
    "# \"val_captions\" - tokenized and numericalized validation captions\n",
    "# \"vocab\" - caption vocabulary, including \"idx_to_token\" and \"token_to_idx\"\n",
    "\n",
    "if os.path.isfile(\"./datasets/coco.pt\"):\n",
    "    print(\"COCO data exists!\")\n",
    "else:\n",
    "    print(\"downloading COCO dataset\")\n",
    "    !wget http://web.eecs.umich.edu/~justincj/teaching/eecs498/coco.pt -P ./datasets/\n",
    "\n",
    "# load COCO data from coco.pt, loaf_COCO is implemented in a5_helper.py\n",
    "data_dict = load_coco_captions(path=\"./datasets/coco.pt\")\n",
    "\n",
    "num_train = data_dict[\"train_images\"].size(0)\n",
    "num_val = data_dict[\"val_images\"].size(0)\n",
    "\n",
    "# declare variables for special tokens\n",
    "NULL_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<NULL>\"]\n",
    "START_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<START>\"]\n",
    "END_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<END>\"]\n",
    "UNK_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80RW_nSH6bhH"
   },
   "source": [
    "## Look at the data\n",
    "It is always a good idea to look at examples from the dataset before working with it.\n",
    "\n",
    "Run the following to sample a small minibatch of training data and show the images and their captions. Running it multiple times and looking at the results helps you to get a sense of the dataset.\n",
    "\n",
    "Note that we decode the captions using the `decode_captions` function.\n",
    "You can check its implementation in `a5_helper.py`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-oiW9Ut6bhH"
   },
   "outputs": [],
   "source": [
    "from a5_helper import decode_captions\n",
    "\n",
    "\n",
    "# Sample a minibatch and show the reshaped 112x112 images and captions\n",
    "sample_idx = torch.randint(0, num_train, (VIS_BATCH_SIZE, ))\n",
    "sample_images = data_dict[\"train_images\"][sample_idx]\n",
    "sample_captions = data_dict[\"train_captions\"][sample_idx]\n",
    "for i in range(VIS_BATCH_SIZE):\n",
    "    plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    caption_str = decode_captions(\n",
    "        sample_captions[i], data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "    plt.title(caption_str)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2SQMNIH6bhJ"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "As discussed in lecture, we will use Recurrent Neural Network (RNN) language models for image captioning. We will cover the vanilla RNN model first and later LSTM and attention-based language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XHZMI356bhJ"
   },
   "source": [
    "## Vanilla RNN: step forward\n",
    "\n",
    "First implement the `rnn_step_forward` for a single timestep of a vanilla recurrent neural network.\n",
    "Run the following to check your implementation. You should see errors on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3oU8JJj6bhK"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import rnn_step_forward\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = torch.linspace(-0.4, 0.7, steps=N * D, **to_double).view(N, D)\n",
    "prev_h = torch.linspace(-0.2, 0.5, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-0.1, 0.9, steps=D * H, **to_double).view(D, H)\n",
    "Wh = torch.linspace(-0.3, 0.7, steps=H * H, **to_double).view(H, H)\n",
    "b = torch.linspace(-0.2, 0.4, steps=H, **to_double)\n",
    "\n",
    "\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = torch.tensor(\n",
    "    [\n",
    "        [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "        [0.66854692, 0.79562378, 0.87755553, 0.92795967],\n",
    "        [0.97934501, 0.99144213, 0.99646691, 0.99854353],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"next_h error: \", rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tid-ljPA6bhL"
   },
   "source": [
    "## Vanilla RNN: step backward\n",
    "Then implement the `rnn_step_backward` for a single timestep of a vanilla recurrent neural network. Run the following to numerically gradient check your implementation. You should see errors on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPyfJofC6bhM"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import rnn_step_backward\n",
    "\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, H = 4, 5, 6\n",
    "x = torch.randn(N, D, **to_double)\n",
    "h = torch.randn(N, H, **to_double)\n",
    "Wx = torch.randn(D, H, **to_double)\n",
    "Wh = torch.randn(H, H, **to_double)\n",
    "b = torch.randn(H, **to_double)\n",
    "\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "\n",
    "dnext_h = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = compute_numeric_gradient(fx, x, dnext_h)\n",
    "dprev_h_num = compute_numeric_gradient(fh, h, dnext_h)\n",
    "dWx_num = compute_numeric_gradient(fWx, Wx, dnext_h)\n",
    "dWh_num = compute_numeric_gradient(fWh, Wh, dnext_h)\n",
    "db_num = compute_numeric_gradient(fb, b, dnext_h)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_step_backward\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_num, dx))\n",
    "print(\"dprev_h error: \", rel_error(dprev_h_num, dprev_h))\n",
    "print(\"dWx error: \", rel_error(dWx_num, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_num, dWh))\n",
    "print(\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjZjH5JW6bhN"
   },
   "source": [
    "## Vanilla RNN: forward\n",
    "Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that processes an entire sequence of data. First implement `rnn_forward` by making calls to the `rnn_step_forward` function that you defined earlier.\n",
    "\n",
    "Run the following to check your implementation. You should see errors on the order of `1e-6` or less.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GQWEn3Z6bhO"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import rnn_forward\n",
    "\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = torch.linspace(-0.1, 0.3, steps=N * T * D, **to_double).view(N, T, D)\n",
    "h0 = torch.linspace(-0.3, 0.1, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-0.2, 0.4, steps=D * H, **to_double).view(D, H)\n",
    "Wh = torch.linspace(-0.4, 0.1, steps=H * H, **to_double).view(H, H)\n",
    "b = torch.linspace(-0.7, 0.1, steps=H, **to_double)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_forward\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.42070749, -0.27279261, -0.11074945, 0.05740409, 0.22236251],\n",
    "            [-0.39525808, -0.22554661, -0.0409454, 0.14649412, 0.32397316],\n",
    "            [-0.42305111, -0.24223728, -0.04287027, 0.15997045, 0.35014525],\n",
    "        ],\n",
    "        [\n",
    "            [-0.55857474, -0.39065825, -0.19198182, 0.02378408, 0.23735671],\n",
    "            [-0.27150199, -0.07088804, 0.13562939, 0.33099728, 0.50158768],\n",
    "            [-0.51014825, -0.30524429, -0.06755202, 0.17806392, 0.40333043],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "print(\"h error: \", rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P570PTsw6bhP"
   },
   "source": [
    "## Vanilla RNN: backward\n",
    "Implement the `rnn_backward` for a vanilla RNN. This should run back-propagation over the entire sequence, making calls to the `rnn_step_backward` function that you defined earlier. You should see errors on the order of `1e-6` or less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny25RusA6bhQ"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import rnn_backward, rnn_forward\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = torch.randn(N, T, D, **to_double)\n",
    "h0 = torch.randn(N, H, **to_double)\n",
    "Wx = torch.randn(D, H, **to_double)\n",
    "Wh = torch.randn(H, H, **to_double)\n",
    "b = torch.randn(H, **to_double)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_backward\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = compute_numeric_gradient(fx, x, dout)\n",
    "dh0_num = compute_numeric_gradient(fh0, h0, dout)\n",
    "dWx_num = compute_numeric_gradient(fWx, Wx, dout)\n",
    "dWh_num = compute_numeric_gradient(fWh, Wh, dout)\n",
    "db_num = compute_numeric_gradient(fb, b, dout)\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_num, dx))\n",
    "print(\"dh0 error: \", rel_error(dh0_num, dh0))\n",
    "print(\"dWx error: \", rel_error(dWx_num, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_num, dWh))\n",
    "print(\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEDUmZOkU_LO"
   },
   "source": [
    "## Vanilla RNN: backward with autograd\n",
    "\n",
    "Now we will entirely depend on the PyTorch autograd module (`torch.autograd`) to compute the backward pass of RNN.\n",
    "`torch.autograd` provides classes and functions implementing **automatic differentiation** of arbitrary scalar valued functions.\n",
    "It requires minimal changes to the existing code - if you pass tensors with `requires_grad=True` to the forward function you wrote earlier, you can just call `.backward(gradient=grad)` on the output to compute gradients on the input and weights.\n",
    "\n",
    "**NOTE: We released the PyTorch API walkthrough notebook during the past assignment, it is available on Piazza and the course website.**\n",
    "\n",
    "Now we can compare the manual backward pass with the autograd backward pass.\n",
    "Read the code in following cell, and execute it to compare your implementation with `torch.autograd`.\n",
    "You should get a relative error less than `1e-10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AMXoqNOVRa_"
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "# set requires_grad=True\n",
    "x = torch.randn(N, T, D, **to_double, requires_grad=True)\n",
    "h0 = torch.randn(N, H, **to_double, requires_grad=True)\n",
    "Wx = torch.randn(D, H, **to_double, requires_grad=True)\n",
    "Wh = torch.randn(H, H, **to_double, requires_grad=True)\n",
    "b = torch.randn(H, **to_double, requires_grad=True)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "# Manual backward:\n",
    "with torch.no_grad():\n",
    "    dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "# Backward with autograd: the magic happens here!\n",
    "out.backward(dout)\n",
    "\n",
    "dx_auto, dh0_auto, dWx_auto, dWh_auto, db_auto = (\n",
    "    x.grad,\n",
    "    h0.grad,\n",
    "    Wx.grad,\n",
    "    Wh.grad,\n",
    "    b.grad,\n",
    ")\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_auto, dx))\n",
    "print(\"dh0 error: \", rel_error(dh0_auto, dh0))\n",
    "print(\"dWx error: \", rel_error(dWx_auto, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_auto, dWh))\n",
    "print(\"db error: \", rel_error(db_auto, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgmxOjX0prA4"
   },
   "source": [
    "## RNN Module\n",
    "\n",
    "We can now wrap the vanilla RNN implementation into a PyTorch module.\n",
    "Recall from the past assignment/tutorial -- `nn.Module` is a base class for all neural network modules in PyTorch. More details regarding its attributes, functions, and methods could be found [in PyTorch documentation](https://pytorch.org/docs/stable/nn.html?highlight=module#torch.nn.Module).\n",
    "\n",
    "In short, the weights and biases are declared in `__init__` and function `forward` will call the `rnn_forward` function from before.\n",
    "The backward function will not be used, and entirely handled by `torch.autograd`.\n",
    "**We have written this part in `RNN` for you but you are highly recommended to go through the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import RNN, rnn_forward\n",
    "\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = torch.randn(N, T, D, **to_double)\n",
    "h0 = torch.randn(N, H, **to_double)\n",
    "\n",
    "rnn_module = RNN(D, H).to(**to_double)\n",
    "\n",
    "# Call forward in module:\n",
    "hn1 = rnn_module(x, h0)\n",
    "\n",
    "# Call without module: (but access weights from module)\n",
    "# Equivalent to above, we won't do this henceforth.\n",
    "Wx, Wh, b = rnn_module.Wx, rnn_module.Wh, rnn_module.b\n",
    "hn2, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "print(\"Output error with/without module: \", rel_error(hn1, hn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIjmnjRd6bhZ"
   },
   "source": [
    "# RNN for image captioning\n",
    "\n",
    "You will implement a few necessary tools and layers in order to build an image captioning model (class `CaptioningRNN`).\n",
    "\n",
    "## Image Feature Extraction\n",
    "\n",
    "The first essential component in an image captioning model is an encoder that inputs an image and produces features for decoding the caption.\n",
    "Here, we use a small [RegNetX-400MF](https://pytorch.org/vision/stable/models.html#torchvision.models.regnet_x_400mf) as the backbone so we can train in reasonable time on Colab. This model is similar to detector backbone seen in the past assignment.\n",
    "\n",
    "It accepts image batches of shape `(B, C, H, W)` and outputs spatial features from final layer that have shape `(B, C, H/32, W/32)`.\n",
    "For vanilla RNN and LSTM, we use the average pooled features (shape `(B, C)`) for decoding captions, whereas for attention LSTM we aggregate the spatial features by learning attention weights.\n",
    "Checkout the `ImageEncoder` method in `rnn_lstm_captioning.py` to see the initialization of the model.\n",
    "\n",
    "We use the implementation from torchvision and put a very thin wrapper module for our use-case.\n",
    "You do not need to implement anything here — you should read and understand the module definition, available in `rnn_lstm_captioning.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pV0Lau_yDwX"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import ImageEncoder\n",
    "\n",
    "model = ImageEncoder(pretrained=True, verbose=True).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVAxU-jO6bhR"
   },
   "source": [
    "## Word embedding\n",
    "In deep learning systems, we commonly represent words using vectors. Each word of the vocabulary will be associated with a vector, and these vectors will be learned jointly with the rest of the system.\n",
    "\n",
    "Implement the `WordEmbedding` module to convert words (represented by integers) into vectors.\n",
    "Run the following to check your implementation. You should see an error on the order of `1e-7` or less. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZuz2ieE6bhR"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import WordEmbedding\n",
    "\n",
    "N, T, V, D = 2, 4, 5, 3\n",
    "\n",
    "x = torch.tensor([[0, 3, 1, 2], [2, 1, 0, 3]]).long()\n",
    "W = torch.linspace(0, 1, steps=V * D, **to_double).view(V, D)\n",
    "\n",
    "# Copy custom weight vector for sanity check:\n",
    "model_emb = WordEmbedding(V, D).to(**to_double)\n",
    "model_emb.W_embed.data.copy_(W)\n",
    "out = model_emb(x)\n",
    "expected_out = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.0, 0.07142857, 0.14285714],\n",
    "            [0.64285714, 0.71428571, 0.78571429],\n",
    "            [0.21428571, 0.28571429, 0.35714286],\n",
    "            [0.42857143, 0.5, 0.57142857],\n",
    "        ],\n",
    "        [\n",
    "            [0.42857143, 0.5, 0.57142857],\n",
    "            [0.21428571, 0.28571429, 0.35714286],\n",
    "            [0.0, 0.07142857, 0.14285714],\n",
    "            [0.64285714, 0.71428571, 0.78571429],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"out error: \", rel_error(expected_out, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6Py13Ak6bhX",
    "tags": []
   },
   "source": [
    "## Temporal Softmax loss\n",
    "\n",
    "In an RNN language model, at every timestep we produce a score for each word in the vocabulary.\n",
    "This score is obtained by applying an affine transform to the hidden state (think `nn.Linear` module).\n",
    "We know the ground-truth word at each timestep, so we use a cross-entropy loss at each timestep.\n",
    "We sum the losses over time and average them over the minibatch.\n",
    "\n",
    "However there is one wrinkle: since we operate over minibatches and different captions may have different lengths, we append `<NULL>` tokens to the end of each caption so they all have the same length. We don't want these `<NULL>` tokens to count toward the loss or gradient, so in addition to scores and ground-truth labels our loss function also accepts a `ignore_index` that tells it which index in caption should be ignored when computing the loss.\n",
    "\n",
    "Implement the `temporal_softmax_loss` and run the following cell to check if the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlFvgXtD6bhX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import temporal_softmax_loss\n",
    "\n",
    "\n",
    "def check_loss(N, T, V, p):\n",
    "    x = 0.001 * torch.randn(N, T, V)\n",
    "    y = torch.randint(V, size=(N, T))\n",
    "    mask = torch.rand(N, T)\n",
    "    y[mask > p] = 0\n",
    "\n",
    "    # YOUR_TURN: Implement temporal_softmax_loss\n",
    "    print(temporal_softmax_loss(x, y, NULL_index).item())\n",
    "\n",
    "\n",
    "check_loss(1000, 1, 10, 1.0)  # Should be about 2.00-2.11\n",
    "check_loss(1000, 10, 10, 1.0)  # Should be about 20.6-21.0\n",
    "check_loss(5000, 10, 10, 0.1)  # Should be about 2.00-2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWrmaSZaUxqX"
   },
   "source": [
    "## Captioning Module\n",
    "\n",
    "Now we are wrapping everything into the captioning module. Implement the `CaptioningRNN` module by following its instructions.\n",
    "This modoule will have a generic structure for RNN, LST, and attention-based LSTM -- which we control by providing `cell_type` argument (one of `[\"rnn\", \"lstm\", \"attn\"]`),\n",
    "For now you only need to implement for the case where `cell_type=\"rnn\"`, you will come back to this module with other two cases later in this assignment.\n",
    "\n",
    "Also skip the inference function (`CaptioningRNN.sample`) for now -- only implement `__init__` and `forward`.\n",
    "Run the following to check your forward pass using a small test case; you should see difference on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8a71FL_6bhZ"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import CaptioningRNN\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, W, H = 10, 400, 30, 40\n",
    "word_to_idx = {\"<NULL>\": 0, \"cat\": 2, \"dog\": 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type=\"rnn\",\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "# Copy parameters for sanity check:\n",
    "for k, v in model.named_parameters():\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).view(*v.shape))\n",
    "\n",
    "images = torch.randn(N, 3, *IMAGE_SHAPE)\n",
    "captions = (torch.arange(N * T) % V).view(N, T)\n",
    "\n",
    "loss = model(images, captions).item()\n",
    "expected_loss = 150.6090393066\n",
    "\n",
    "print(\"loss: \", loss)\n",
    "print(\"expected loss: \", expected_loss)\n",
    "print(\"difference: \", rel_error(torch.tensor(loss), torch.tensor(expected_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YAOcQ4h6bhc"
   },
   "source": [
    "## Overfit small data\n",
    "\n",
    "To make sure that everything is working as expected, we can try to overfit this image captioning model to a small subset of data.\n",
    "\n",
    "We have implemented the `train_captioner` function which accepts the model and training data, and runs a simple training loop - passing data to model, collecting training loss, then calling `backward()` to obtain gradients. These gradients are optimized using the [AdamW optimizer](https://arxiv.org/abs/1711.05101) (supported by PyTorch).\n",
    "You can read its implementation in `a5_helper.py`. \n",
    "\n",
    "We will overfit on a subset of 50 examples.\n",
    "You should see a final loss of less than `0.5` and it should be done fairly quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzhsGRzk6bhd"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = 50\n",
    "sample_idx = torch.linspace(0, num_train - 1, steps=small_num_train).long()\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# optimization arguments\n",
    "num_epochs = 80\n",
    "\n",
    "# create the image captioning model\n",
    "model = CaptioningRNN(\n",
    "    cell_type=\"rnn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "model = model.to(**to_float)\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    rnn_overfit, _ = train_captioner(\n",
    "        model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=OVR_BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiHsRysE6bhe"
   },
   "source": [
    "## Inference: Sampling Captions\n",
    "\n",
    "Unlike classification models, image captioning models behave very differently at training time and at test time.\n",
    "At training time, we have access to the ground-truth caption, so we feed ground-truth words as input to the RNN at each timestep.\n",
    "At test time, we sample from the distribution over the vocabulary at each timestep, and feed the sample as input to the RNN at the next timestep.\n",
    "\n",
    "Implement the `CaptioningRNN.sample` for test-time sampling. After doing so, run the following to train a captioning model and sample from the model on both training and validation data.\n",
    "\n",
    "### Train the image captioning model\n",
    "\n",
    "Now perform the training on the entire training set. You should see a final loss less than `2.0` and each epoch should take ~14s - 44s to run, depending on the GPU colab assigns you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXHnPuM_FU7k"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = num_train\n",
    "sample_idx = torch.randint(num_train, size=(small_num_train,))\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "rnn_model = CaptioningRNN(\n",
    "    cell_type=\"rnn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    rnn_model_submit, rnn_loss_submit = train_captioner(\n",
    "        rnn_model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=60,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97xga3Q5GO8B"
   },
   "source": [
    "### Test-time sampling\n",
    "The samples on training data should be very good; the samples on validation data will probably make less sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rvt326nX6bhf"
   },
   "outputs": [],
   "source": [
    "from a5_helper import decode_captions\n",
    "\n",
    "\n",
    "rnn_model.eval()\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    sample_idx = torch.randint(\n",
    "        0, num_train if split == \"train\" else num_val, (VIS_BATCH_SIZE,)\n",
    "    )\n",
    "    sample_images = data_dict[split + \"_images\"][sample_idx]\n",
    "    sample_captions = data_dict[split + \"_captions\"][sample_idx]\n",
    "\n",
    "    # decode_captions is loaded from a5_helper.py\n",
    "    gt_captions = decode_captions(sample_captions, data_dict[\"vocab\"][\"idx_to_token\"])\n",
    "\n",
    "    generated_captions = rnn_model.sample(sample_images.to(DEVICE))\n",
    "    generated_captions = decode_captions(\n",
    "        generated_captions, data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "\n",
    "    for i in range(VIS_BATCH_SIZE):\n",
    "        plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"[{split}] RNN Generated: {generated_captions[i]}\\nGT: {gt_captions[i]}\"\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8Zd6FGPvMMa"
   },
   "source": [
    "# Image Captioning with LSTMs\n",
    "\n",
    "So far you have implemented a vanilla RNN and applied it to image captioning.\n",
    "Next we will implement LSTM and use it for image captioning.\n",
    "\n",
    "**LSTM** stands for [Long-Short Term Memory Networks](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory), a variant of vanilla Recurrent Neural Networks.\n",
    "Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradients caused by repeated matrix multiplication.\n",
    "LSTMs solve this problem by replacing the simple update rule of the vanilla RNN with a gating mechanism.\n",
    "\n",
    "**LSTM Update Rule:** Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$; the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
    "\n",
    "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "i = \\sigma(a_i) \\hspace{2pc}\n",
    "f = \\sigma(a_f) \\hspace{2pc}\n",
    "o = \\sigma(a_o) \\hspace{2pc}\n",
    "g = \\tanh(a_g)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
    "\n",
    "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
    "\n",
    "$$\n",
    "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n",
    "h_t = o\\odot\\tanh(c_t)\n",
    "$$\n",
    "\n",
    "where $\\odot$ is the elementwise product of vectors.\n",
    "\n",
    "In the rest of the notebook we will implement the LSTM update rule and apply it to the image captioning task.\n",
    "In the code, we assume that data is stored in batches so that $X_t \\in \\mathbb{R}^{N\\times D}$, and will work with *transposed* versions of the parameters: $W_x \\in \\mathbb{R}^{D \\times 4H}$, $W_h \\in \\mathbb{R}^{H\\times 4H}$ so that activations $A \\in \\mathbb{R}^{N\\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4DNkZYevMMc"
   },
   "source": [
    "## LSTM: step forward\n",
    "\n",
    "Implement the forward pass for a single timestep of an LSTM in the `LSTM.step_forward()` function.\n",
    "This should be similar to the `rnn_step_forward` function that you implemented above, but using the LSTM update rule instead.\n",
    "Since `LSTM` extends PyTorch `nn.Module`, you don't need to implement backward part!\n",
    "\n",
    "Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awLF_A5ZvMMd"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import LSTM\n",
    "\n",
    "\n",
    "N, D, H = 3, 4, 5\n",
    "x = torch.linspace(-0.4, 1.2, steps=N * D, **to_double).view(N, D)\n",
    "prev_h = torch.linspace(-0.3, 0.7, steps=N * H, **to_double).view(N, H)\n",
    "prev_c = torch.linspace(-0.4, 0.9, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-2.1, 1.3, steps=4 * D * H, **to_double).view(D, 4 * H)\n",
    "Wh = torch.linspace(-0.7, 2.2, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "b = torch.linspace(0.3, 0.7, steps=4 * H, **to_double)\n",
    "\n",
    "\n",
    "# Create module and copy weight tensors for sanity check:\n",
    "model = LSTM(D, H).to(**to_double)\n",
    "model.Wx.data.copy_(Wx)\n",
    "model.Wh.data.copy_(Wh)\n",
    "model.b.data.copy_(b)\n",
    "\n",
    "next_h, next_c = model.step_forward(x, prev_h, prev_c)\n",
    "\n",
    "expected_next_h = torch.tensor(\n",
    "    [\n",
    "        [0.24635157, 0.28610883, 0.32240467, 0.35525807, 0.38474904],\n",
    "        [0.49223563, 0.55611431, 0.61507696, 0.66844003, 0.7159181],\n",
    "        [0.56735664, 0.66310127, 0.74419266, 0.80889665, 0.858299],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "expected_next_c = torch.tensor(\n",
    "    [\n",
    "        [0.32986176, 0.39145139, 0.451556, 0.51014116, 0.56717407],\n",
    "        [0.66382255, 0.76674007, 0.87195994, 0.97902709, 1.08751345],\n",
    "        [0.74192008, 0.90592151, 1.07717006, 1.25120233, 1.42395676],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"next_h error: \", rel_error(expected_next_h, next_h))\n",
    "print(\"next_c error: \", rel_error(expected_next_c, next_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErgRQwwzvMMt"
   },
   "source": [
    "## LSTM: forward\n",
    "\n",
    "Implement the `LSTM.forward()` function to run an LSTM forward on an entire time-series of data.\n",
    "When you are done, run the following to check your implementation. You should see an error on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_x-3BJiEvMMv"
   },
   "outputs": [],
   "source": [
    "N, D, H, T = 2, 5, 4, 3\n",
    "x = torch.linspace(-0.4, 0.6, steps=N * T * D, **to_double).view(N, T, D)\n",
    "h0 = torch.linspace(-0.4, 0.8, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-0.2, 0.9, steps=4 * D * H, **to_double).view(D, 4 * H)\n",
    "Wh = torch.linspace(-0.3, 0.6, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "b = torch.linspace(0.2, 0.7, steps=4 * H, **to_double)\n",
    "\n",
    "\n",
    "# Create module and copy weight tensors for sanity check:\n",
    "model = LSTM(D, H).to(**to_double)\n",
    "model.Wx.data.copy_(Wx)\n",
    "model.Wh.data.copy_(Wh)\n",
    "model.b.data.copy_(b)\n",
    "\n",
    "hn = model(x, h0)\n",
    "\n",
    "expected_hn = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.01764008, 0.01823233, 0.01882671, 0.0194232],\n",
    "            [0.11287491, 0.12146228, 0.13018446, 0.13902939],\n",
    "            [0.31358768, 0.33338627, 0.35304453, 0.37250975],\n",
    "        ],\n",
    "        [\n",
    "            [0.45767879, 0.4761092, 0.4936887, 0.51041945],\n",
    "            [0.6704845, 0.69350089, 0.71486014, 0.7346449],\n",
    "            [0.81733511, 0.83677871, 0.85403753, 0.86935314],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"hn error: \", rel_error(expected_hn, hn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92SStL_tvMNK"
   },
   "source": [
    "## LSTM captioning model\n",
    "\n",
    "Now that you have implemented the `LSTM` module, update the `CaptioningRNN` module (`__init__` and `forward` implementation method **ONLY**) to also handle the case where `self.cell_type` is `lstm`.\n",
    "**This should require adding less than 5 lines of code.**\n",
    "\n",
    "Once you have done so, run the following to check your implementation. You should see a difference on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNpiC4WSvMNL"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import CaptioningRNN\n",
    "\n",
    "N, D, W, H = 10, 400, 30, 40\n",
    "word_to_idx = {\"<NULL>\": 0, \"cat\": 2, \"dog\": 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "# YOUR_TURN: Implement CaptioningRNN for lstm\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type=\"lstm\",\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "for k, v in model.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).view(*v.shape))\n",
    "\n",
    "images = torch.linspace(\n",
    "    -3.0, 3.0, steps=(N * 3 * IMAGE_SHAPE[0] * IMAGE_SHAPE[1]), **to_float\n",
    ").view(N, 3, *IMAGE_SHAPE)\n",
    "captions = (torch.arange(N * T) % V).view(N, T)\n",
    "\n",
    "loss = model(images.to(DEVICE), captions.to(DEVICE))\n",
    "expected_loss = torch.tensor(146.3161468505)\n",
    "\n",
    "print(\"loss: \", loss.item())\n",
    "print(\"expected loss: \", expected_loss.item())\n",
    "print(\"difference: \", rel_error(loss, expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06hbDnRXvMNO"
   },
   "source": [
    "## Overfit small data\n",
    "We have written this part for you. Run the following to overfit an LSTM captioning model on the same small dataset as we used for the RNN previously. You should see a final loss less than `4` after 80 epochs and it should run fairly quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-tETnd3vMNP"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Data input.\n",
    "small_num_train = 50\n",
    "sample_idx = torch.linspace(0, num_train - 1, steps=small_num_train).long()\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx].to(DEVICE)\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx].to(DEVICE)\n",
    "\n",
    "# Create the image captioning model.\n",
    "model = CaptioningRNN(\n",
    "    cell_type=\"lstm\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "for learning_rate in [1e-2]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    lstm_overfit, _ = train_captioner(\n",
    "        model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=80,\n",
    "        batch_size=OVR_BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vzLUzlWvMNT"
   },
   "source": [
    "## Caption sampling\n",
    "\n",
    "Modify the  `CaptioningRNN.sample` method in class to handle the case where `self.cell_type` is `lstm`. **This should take fewer than 10 lines of code.**\n",
    "When you are done, run the following cells to train the captioning model first, then sample some captions from your model during test time.\n",
    "\n",
    "### Train the net\n",
    "\n",
    "Perform the training on the entire training set. You should see a final loss less than `1.8`. Each epoch should take ~7s - 14s to run, depending on the GPU\n",
    "colab assigns you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9MFRowdoHW7"
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = num_train\n",
    "sample_idx = torch.randint(num_train, size=(small_num_train,))\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "lstm_model = CaptioningRNN(\n",
    "    cell_type=\"lstm\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "lstm_model = lstm_model.to(DEVICE)\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    lstm_model_submit, lstm_loss_submit = train_captioner(\n",
    "        lstm_model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=60,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsM2pIYpG3v1"
   },
   "source": [
    "### Test-time sampling\n",
    "As with the RNN, the samples on training data should be very good; the samples on validation data will probably make less sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziQJ7SBnvMNU"
   },
   "outputs": [],
   "source": [
    "from a5_helper import decode_captions\n",
    "\n",
    "\n",
    "lstm_model.eval()\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    sample_idx = torch.randint(\n",
    "        0, num_train if split == \"train\" else num_val, (VIS_BATCH_SIZE,)\n",
    "    )\n",
    "    sample_images = data_dict[split + \"_images\"][sample_idx]\n",
    "    sample_captions = data_dict[split + \"_captions\"][sample_idx]\n",
    "\n",
    "    # decode_captions is loaded from a5_helper.py\n",
    "    gt_captions = decode_captions(sample_captions, data_dict[\"vocab\"][\"idx_to_token\"])\n",
    "    lstm_model.eval()\n",
    "    generated_captions = lstm_model.sample(sample_images.to(DEVICE))\n",
    "    generated_captions = decode_captions(\n",
    "        generated_captions, data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "\n",
    "    for i in range(VIS_BATCH_SIZE):\n",
    "        plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"[{split}] LSTM Generated: {generated_captions[i]}\\nGT: {gt_captions[i]}\"\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECcPPE_Pqc8v"
   },
   "source": [
    "# Attention LSTM\n",
    "Attention LSTM essentially adds an attention input $x_{attn}^t\\in\\mathbb{R}^H$ into LSTM, along with $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$.\n",
    "\n",
    "To get the attention input $x_{attn}^t$, here we adopt a method called `scaled dot-product attention`, as covered in the lecture. We first project the CNN feature activation from $\\mathbb{R}^{400\\times4\\times4}$ to $\\mathbb{R}^{H\\times4\\times4}$ using an affine layer. Given the projected activation $A\\in \\mathbb{R}^{H\\times4\\times4}$ and the LSTM hidden state from the previous time step $h_{t-1}$, we formuate the attention weights on $A$ at time step $t$ as $M_{attn}^t=h_{t-1}A/\\sqrt{H} \\in \\mathbb{R}^{4\\times4}$.\n",
    "\n",
    "To simplify the formuation here, we flatten the spatial dimensions of $A$ and $M_{attn}^t$ which gives $\\tilde{A}\\in \\mathbb{R}^{H\\times16}$ and $\\tilde{M^t}_{attn}=h_{t-1}A\\in \\mathbb{R}^{16}$.\n",
    "We add a **`softmax`** activation function on $\\tilde{M^t}_{attn}$ so that the attention weights at each time step are normalized and sum up to one.\n",
    "\n",
    "The attention embedding given the attention weights is then $x_{attn}^t=\\tilde{A}\\tilde{M^t}_{attn} \\in\\mathbb{R}^H$. Next, you will implement a batch version of the attention layer we have described here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTDk54Q4ubQ1"
   },
   "source": [
    "## Scaled dot-product attention\n",
    "Implement the `dot_product_attention` function. Given the LSTM hidden state from the previous time step `prev_h` (or $h_{t-1}$) and the projected CNN feature activation `A`, compute the attention weights `attn_weights` (or $\\tilde{M^t}_{attn}$ with a reshaping to $\\mathbb{R}^{4\\times4}$) attention embedding output `attn` (or $x_{attn}^t$) using the formulation we provided.\n",
    "\n",
    "When you are done, run the following to check your implementation. You should see an error on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irAslXWfaVGw"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import dot_product_attention\n",
    "\n",
    "\n",
    "N, H = 2, 5\n",
    "D_a = 4\n",
    "\n",
    "prev_h = torch.linspace(-0.4, 0.6, steps=N * H, **to_double).view(N, H)\n",
    "A = torch.linspace(-0.4, 1.8, steps=N * H * D_a * D_a, **to_double).view(\n",
    "    N, H, D_a, D_a\n",
    ")\n",
    "\n",
    "# YOUR_TURN: Implement dot_product_attention\n",
    "attn, attn_weights = dot_product_attention(prev_h, A)\n",
    "\n",
    "expected_attn = torch.tensor(\n",
    "    [\n",
    "        [-0.29784344, -0.07645979, 0.14492386, 0.36630751, 0.58769115],\n",
    "        [0.81412643, 1.03551008, 1.25689373, 1.47827738, 1.69966103],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "expected_attn_weights = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.06511126, 0.06475411, 0.06439892, 0.06404568],\n",
    "            [0.06369438, 0.06334500, 0.06299754, 0.06265198],\n",
    "            [0.06230832, 0.06196655, 0.06162665, 0.06128861],\n",
    "            [0.06095243, 0.06061809, 0.06028559, 0.05995491],\n",
    "        ],\n",
    "        [\n",
    "            [0.05717142, 0.05784357, 0.05852362, 0.05921167],\n",
    "            [0.05990781, 0.06061213, 0.06132473, 0.06204571],\n",
    "            [0.06277517, 0.06351320, 0.06425991, 0.06501540],\n",
    "            [0.06577977, 0.06655312, 0.06733557, 0.06812722],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"attn error: \", rel_error(expected_attn, attn))\n",
    "print(\"attn_weights error: \", rel_error(expected_attn_weights, attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVddQlj4xwRk"
   },
   "source": [
    "## Attention LSTM: step forward\n",
    "\n",
    "Implement `AttentionLSTM.step_forward()` by following its instructions and input specifications.\n",
    "It is mostly similar to `LSTM.step_forward()` but has extra attention input `attn` (or $x_{attn}$) and its embedding weight matrix `Wattn` (or $W_{attn}$),\n",
    "these are defined in `AttentionLSTM.__init__()`.\n",
    "Hence, at each timestep the *activation vector* $a\\in\\mathbb{R}^{4H}$ in LSTM cell is formulated as:\n",
    "\n",
    "$a=W_xx_t + W_hh_{t-1}+W_{attn}x_{attn}^t+b$.\n",
    "\n",
    "\n",
    "**This should require adding less than 5 lines of code.**\n",
    "Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaS31Ncf3l0d"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import AttentionLSTM\n",
    "\n",
    "\n",
    "N, D, H = 3, 4, 5\n",
    "\n",
    "x = torch.linspace(-0.4, 1.2, steps=N * D, **to_double).view(N, D)\n",
    "prev_h = torch.linspace(-0.3, 0.7, steps=N * H, **to_double).view(N, H)\n",
    "prev_c = torch.linspace(-0.4, 0.9, steps=N * H, **to_double).view(N, H)\n",
    "attn = torch.linspace(0.6, 1.8, steps=N * H, **to_double).view(N, H)\n",
    "\n",
    "Wx = torch.linspace(-2.1, 1.3, steps=4 * D * H, **to_double).view(D, 4 * H)\n",
    "Wh = torch.linspace(-0.7, 2.2, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "b = torch.linspace(0.3, 0.7, steps=4 * H, **to_double)\n",
    "Wattn = torch.linspace(1.3, 4.2, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "\n",
    "# Create module and copy weight tensors for sanity check:\n",
    "model = AttentionLSTM(D, H).to(**to_double)\n",
    "model.Wx.data.copy_(Wx)\n",
    "model.Wh.data.copy_(Wh)\n",
    "model.b.data.copy_(b)\n",
    "model.Wattn.data.copy_(Wattn)\n",
    "\n",
    "next_h, next_c = model.step_forward(x, prev_h, prev_c, attn)\n",
    "\n",
    "\n",
    "expected_next_h = torch.tensor(\n",
    "    [\n",
    "        [0.53704256, 0.59980774, 0.65596820, 0.70569729, 0.74932626],\n",
    "        [0.78729857, 0.82010653, 0.84828362, 0.87235677, 0.89283167],\n",
    "        [0.91017981, 0.92483119, 0.93717126, 0.94754073, 0.95623746],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "expected_next_c = torch.tensor(\n",
    "    [\n",
    "        [0.59999328, 0.69285041, 0.78570758, 0.87856479, 0.97142202],\n",
    "        [1.06428558, 1.15714276, 1.24999992, 1.34285708, 1.43571424],\n",
    "        [1.52857143, 1.62142857, 1.71428571, 1.80714286, 1.90000000],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"next_h error: \", rel_error(expected_next_h, next_h))\n",
    "print(\"next_c error: \", rel_error(expected_next_c, next_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRry6hvQ7ywx"
   },
   "source": [
    "## Attention LSTM: forward\n",
    "\n",
    "Now, implement the `AttentinLSTM.forward()` function to run an attention-based LSTM on an entire timeseries of data.\n",
    "You will have to use the `dot_product_attention` function from outside this module.\n",
    "\n",
    "When you are done, run the following to check your implementation. You should see an error on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB6VU8nl4SmS"
   },
   "outputs": [],
   "source": [
    "N, D, H, T = 2, 5, 4, 3\n",
    "D_a = 4\n",
    "\n",
    "x = torch.linspace(-0.4, 0.6, steps=N * T * D, **to_double).view(N, T, D)\n",
    "A = torch.linspace(-0.4, 1.8, steps=N * H * D_a * D_a, **to_double).view(\n",
    "    N, H, D_a, D_a\n",
    ")\n",
    "\n",
    "Wx = torch.linspace(-0.2, 0.9, steps=4 * D * H, **to_double).view(D, 4 * H)\n",
    "Wh = torch.linspace(-0.3, 0.6, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "Wattn = torch.linspace(1.3, 4.2, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "b = torch.linspace(0.2, 0.7, steps=4 * H, **to_double)\n",
    "\n",
    "\n",
    "# Create module and copy weight tensors for sanity check:\n",
    "model = AttentionLSTM(D, H).to(**to_double)\n",
    "model.Wx.data.copy_(Wx)\n",
    "model.Wh.data.copy_(Wh)\n",
    "model.b.data.copy_(b)\n",
    "model.Wattn.data.copy_(Wattn)\n",
    "\n",
    "# YOUR_TURN: Implement attention_forward\n",
    "hn = model(x, A)\n",
    "\n",
    "expected_hn = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.56141729, 0.70274849, 0.80000386, 0.86349400],\n",
    "            [0.89556391, 0.92856726, 0.94950579, 0.96281018],\n",
    "            [0.96792077, 0.97535465, 0.98039623, 0.98392994],\n",
    "        ],\n",
    "        [\n",
    "            [0.95065880, 0.97135490, 0.98344373, 0.99045552],\n",
    "            [0.99317679, 0.99607466, 0.99774317, 0.99870293],\n",
    "            [0.99907382, 0.99946784, 0.99969426, 0.99982435],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"h error: \", rel_error(expected_hn, hn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VzpyHuX6Jzc"
   },
   "source": [
    "## Attention LSTM captioning model\n",
    "\n",
    "With all your implementation done so far, you can finally update the implementation of `CaptioningRNN.__init__` and `CaptioningRNN.forward` methods once again.\n",
    "This time, handle the case where `self.cell_type` is `attn`. **This should require adding less than 10 lines of code.**\n",
    "\n",
    "Once you have done so, run the following to check your implementation. You should see a difference on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VqGqDYw6Jzd"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import CaptioningRNN\n",
    "\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, W, H = 10, 400, 30, 40\n",
    "word_to_idx = {\"<NULL>\": 0, \"cat\": 2, \"dog\": 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "# YOUR_TURN: Modify CaptioningRNN for attention\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type=\"attn\",\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "for k, v in model.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).view(*v.shape))\n",
    "\n",
    "images = torch.linspace(\n",
    "    -3.0, 3.0, steps=(N * 3 * IMAGE_SHAPE[0] * IMAGE_SHAPE[1])\n",
    ").view(N, 3, *IMAGE_SHAPE)\n",
    "captions = (torch.arange(N * T) % V).view(N, T)\n",
    "\n",
    "loss = model(images.to(DEVICE), captions.to(DEVICE))\n",
    "expected_loss = torch.tensor(8.0156393051)\n",
    "\n",
    "print(\"loss: \", loss.item())\n",
    "print(\"expected loss: \", expected_loss.item())\n",
    "print(\"difference: \", rel_error(loss, expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYxXTAn4q0wV"
   },
   "source": [
    "## Overfit small data\n",
    "We have written this part for you. Run the following to overfit an Attention LSTM captioning model on the same small dataset as we used for the RNN previously. You should see a final loss less than `9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlK7lKUgWeDS"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = 50\n",
    "sample_idx = torch.linspace(0, num_train - 1, steps=small_num_train).long()\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "model = CaptioningRNN(\n",
    "    cell_type=\"attn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    attn_overfit, _ = train_captioner(\n",
    "        model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=80,\n",
    "        batch_size=OVR_BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ircMb7_qq7vB"
   },
   "source": [
    "## Caption sampling\n",
    "\n",
    "Modify the `CaptioningRNN.sample` method to handle the case where `self.cell_type` is `attn`. **This should take fewer than 10 lines of code.**\n",
    "\n",
    "When you are done run the following to train a captioning model and sample from the model on some training and validation set samples.\n",
    "\n",
    "### Train the net\n",
    "\n",
    "Now, perform the training on the entire training set. You should see a final loss less than `0.5`. Each epoch should take ~8s to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScBvAfcXdVv4"
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = num_train\n",
    "sample_idx = torch.randint(num_train, size=(small_num_train,))\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "attn_model = CaptioningRNN(\n",
    "    cell_type=\"attn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "attn_model = attn_model.to(DEVICE)\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    attn_model_submit, attn_loss_submit = train_captioner(\n",
    "        attn_model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=60,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ham_O1TG_z7"
   },
   "source": [
    "### Test-time sampling and visualization\n",
    "As with RNN and LSTM, the samples on training data should be very good; the samples on validation data will probably make less sense.\n",
    "\n",
    "We use the `attention_visualizer` function from `eecs598/utils.py` to visualize the attended regions per generated word. Note that sometimes the attended regions (brighter) might not make much sense particially due to our low resolution image input. In real applications, the attended regions are more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i8KNWSDSLNu"
   },
   "outputs": [],
   "source": [
    "# Sample a minibatch and show the reshaped 112x112 images,\n",
    "# GT captions, and generated captions by your model.\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    sample_idx = torch.randint(\n",
    "        0, num_train if split == \"train\" else num_val, (VIS_BATCH_SIZE,)\n",
    "    )\n",
    "    sample_images = data_dict[split + \"_images\"][sample_idx]\n",
    "    sample_captions = data_dict[split + \"_captions\"][sample_idx]\n",
    "\n",
    "    # decode_captions is loaded from a5_helper.py\n",
    "    gt_captions = decode_captions(sample_captions, data_dict[\"vocab\"][\"idx_to_token\"])\n",
    "    attn_model.eval()\n",
    "    generated_captions, attn_weights_all = attn_model.sample(sample_images.to(DEVICE))\n",
    "    generated_captions = decode_captions(\n",
    "        generated_captions, data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "\n",
    "    for i in range(VIS_BATCH_SIZE):\n",
    "        plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            \"%s\\nAttention LSTM Generated:%s\\nGT:%s\"\n",
    "            % (split, generated_captions[i], gt_captions[i])\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        tokens = generated_captions[i].split(\" \")\n",
    "\n",
    "        vis_attn = []\n",
    "        for j in range(len(tokens)):\n",
    "            img = sample_images[i]\n",
    "            attn_weights = attn_weights_all[i][j]\n",
    "            token = tokens[j]\n",
    "            img_copy = attention_visualizer(img, attn_weights, token)\n",
    "            vis_attn.append(transforms.ToTensor()(img_copy))\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (20.0, 20.0)\n",
    "        vis_attn = make_grid(vis_attn, nrow=8)\n",
    "        plt.imshow(torch.flip(vis_attn, dims=(0,)).permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghB8BwfUpmI5"
   },
   "source": [
    "# Save results for submission\n",
    "\n",
    "Once you have finished all your implementation, run \"Runtime -> Restart and run all...\" to re-run all cells and display outputs.\n",
    "Make sure all outputs are displayed properly and the outputs are same as what you expected!\n",
    "\n",
    "Once all the cells are completed, execute the following cell to save the final losses for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_21vKQeRMpC"
   },
   "outputs": [],
   "source": [
    "submission = {\n",
    "    \"rnn_losses\": rnn_loss_submit,\n",
    "    \"lstm_losses\": lstm_loss_submit,\n",
    "    \"attn_losses\": attn_loss_submit,\n",
    "}\n",
    "submission_path = os.path.join(GOOGLE_DRIVE_PATH, \"rnn_lstm_attention_submission.pt\")\n",
    "torch.save(submission, submission_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_lstm_attention_captioning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
